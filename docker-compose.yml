version: "3.9"

services:
  runtime:
    build:
      context: ./runtime
      dockerfile: Dockerfile
    image: openhands-runtime-dotnet
    container_name: openhands-runtime
    stdin_open: true
    tty: true
    volumes:
      - ./workspace:/workspace
    networks:
      - openhands-net
    extra_hosts:
      - "host.docker.internal:host-gateway"

  openhands:
    image: docker.all-hands.dev/all-hands-ai/openhands:0.54
    container_name: openhands-hands-app
    pull_policy: always
    environment:
      LOG_ALL_EVENTS: "true"
      LLM_MAX_INPUT_TOKENS: 16384
      LLM_MAX_OUTPUT_TOKENS: 16384
      OPENHANDS_LLM_PROVIDER: ollama
      OPENHANDS_LLM_MODEL: devstral:latest
      OPENHANDS_LLM_ENDPOINT: http://ollama:11434
      SANDBOX_RUNTIME_CONTAINER_IMAGE: openhands-runtime-dotnet
    ports:
      - "3000:3000"
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      - ./openhands:/.openhands
      - ./workspace:/workspace
    extra_hosts:
      - "host.docker.internal:host-gateway"
    depends_on:
      runtime:
        condition: service_started
      ollama:
        condition: service_started
    networks:
      - openhands-net
    deploy:
      resources:
        limits:
          memory: 4g
          cpus: '2.0'
        reservations:
          memory: 2g
          cpus: '1.0'

  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    environment:
      - OLLAMA_CONTEXT_LENGTH=32768
      - OLLAMA_HOST=0.0.0.0:11434
      - OLLAMA_MAX_LOADED_MODELS=1
      - OLLAMA_MODEL=devstral:latest
      - OLLAMA_NUM_PARALLEL=1
      - OLLAMA_KEEP_ALIVE=-1
      - OLLAMA_FLASH_ATTENTION=1
      - OLLAMA_LOAD_TIMEOUT=900
      - OLLAMA_GPU_OVERHEAD=2147483648
    ports:
      - "11434:11434"
    volumes:
      - ./ollama:/root/.ollama
      - ./workspace:/workspace
    restart: unless-stopped
    entrypoint: ["/bin/sh", "-c"]
    command:
      - >
        ollama serve &
        sleep 5 &&
        ollama pull devstral:latest &&
        wait
    networks:
      - openhands-net
    extra_hosts:
      - "host.docker.internal:host-gateway"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    runtime: nvidia
        
  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: open-webui
    ports:
      - "8080:8080"
    volumes:
      - ./open-webui:/app/backend/data
      - ./workspace:/workspace
    environment:
      OLLAMA_BASE_URL: http://ollama:11434
      OPENWEBUI_USERNAME: admin@admin.com
      OPENWEBUI_PASSWORD: admin
    networks:
      - openhands-net
    extra_hosts:
      - "host.docker.internal:host-gateway"
    depends_on:
      - ollama
    deploy:
      resources:
        limits:
          memory: 2g
          cpus: '1.0'
        reservations:
          memory: 512m
          cpus: '0.5'

networks:
  openhands-net:
    name: openhands-net
