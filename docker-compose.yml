version: "3.9"

services:
  runtime:
    build:
      context: ./runtime
      dockerfile: Dockerfile
    image: openhands-runtime-dotnet
    container_name: openhands-runtime
    stdin_open: true
    tty: true
    volumes:
      - ./workspace:/workspace
    networks:
      - openhands-net
    extra_hosts:
      - "host.docker.internal:host-gateway"

  openhands:
    image: docker.all-hands.dev/all-hands-ai/openhands:0.52
    container_name: openhands-app
    pull_policy: always
    stdin_open: true
    tty: true
    environment:
      LOG_ALL_EVENTS: "true"
      LLM_MAX_INPUT_TOKENS: 90000
      LLM_MAX_OUTPUT_TOKENS: 4096
      OLLAMA_CONTEXT_LENGTH: 4096
      SANDBOX_VOLUMES: ./workspace:/workspace:rw
      SANDBOX_RUNTIME_CONTAINER_IMAGE: openhands-runtime-dotnet
    ports:
      - "3000:3000"
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      - ./openhands:/.openhands
      - ./workspace:/workspace
    extra_hosts:
      - "host.docker.internal:host-gateway"
    depends_on:
      runtime:
        condition: service_started
      ollama:
        condition: service_started
    networks:
      - openhands-net
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    environment:
      - OLLAMA_CONTEXT_LENGTH=4096
      - OLLAMA_HOST=0.0.0.0:11434
      - OLLAMA_MAX_LOADED_MODELS=1
      - OLLAMA_NUM_PARALLEL=2
      - OLLAMA_KEEP_ALIVE=30m
      - OLLAMA_GPU_OVERHEAD=2147483648
    ports:
      - "11434:11434"
    volumes:
      - ./ollama:/root/.ollama
      - ./workspace:/workspace
    restart: unless-stopped
    entrypoint: ["/bin/sh", "-c"]
    command:
      - >
        ollama serve &
        sleep 5 &&
        ollama pull codellama:7b &&
        wait
    networks:
      - openhands-net
    extra_hosts:
      - "host.docker.internal:host-gateway"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
      
  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: open-webui
    ports:
      - "8080:8080"
    volumes:
      - ./open-webui:/app/backend/data
      - ./workspace:/workspace
    environment:
      OLLAMA_BASE_URL: http://ollama:11434
      OPENWEBUI_USERNAME: admin@admin.com
      OPENWEBUI_PASSWORD: admin
    networks:
      - openhands-net
    extra_hosts:
      - "host.docker.internal:host-gateway"
    depends_on:
      - ollama

  vscode:
    image: ghcr.io/linuxserver/code-server
    container_name: vscode
    environment:
      - PUID=1000
      - PGID=1000
      - TZ=America/Sao_Paulo
      - OLLAMA_BASE_URL=http://ollama:11434
      - HOST_IP=host.docker.internal
      - DOCKER_HOST_IP=host.docker.internal
    ports:
      - "8443:8443"
    volumes:
      - ./vscode:/config
      - ./workspace:/workspace
    depends_on:
      - ollama
    networks:
      - openhands-net
    extra_hosts:
      - "host.docker.internal:host-gateway"
    
networks:
  openhands-net:
    name: openhands-net
