version: "3.9"

services:
  runtime:
    build:
      context: ./runtime
      dockerfile: Dockerfile
    image: openhands-runtime-dotnet
    container_name: openhands-runtime
    stdin_open: true
    tty: true
    volumes:
      - ./workspace:/workspace
    networks:
      - openhands-net
    extra_hosts:
      - "host.docker.internal:host-gateway"

  openhands:
    image: docker.all-hands.dev/all-hands-ai/openhands:0.55
    container_name: openhands-hands-app
    pull_policy: always
    environment:
      LOG_ALL_EVENTS: "true"
      # ConfiguraÃ§Ãµes do LLM - MODELO OTIMIZADO
      OPENHANDS_LLM_PROVIDER: ollama
      OPENHANDS_LLM_MODEL: qwen2.5-coder:7b  # âœ… MELHOR QUE CODELLAMA
      OPENHANDS_LLM_ENDPOINT: http://ollama:11434
      OLLAMA_MODEL: qwen2.5-coder:7b
      # OtimizaÃ§Ãµes GPU/CPU
      CUDA_VISIBLE_DEVICES: 0
      OPENHANDS_LLM_GPU_LAYERS: 35
      OPENHANDS_LLM_CTX_SIZE: 32768
      # Performance melhorada
      OPENHANDS_LOAD_TIMEOUT: 900
      OPENHANDS_MAX_PARALLEL_REQUESTS: 1
      OPENHANDS_MEMORY_BUDGET: 8589934592
      SANDBOX_RUNTIME_CONTAINER_IMAGE: openhands-runtime-dotnet
      # âœ… NOVAS OTIMIZAÃ‡Ã•ES
      OPENHANDS_LLM_TEMPERATURE: 0.1        # Mais determinÃ­stico para cÃ³digo
      OPENHANDS_LLM_TOP_P: 0.9
      OPENHANDS_MAX_ITERATIONS: 50          # Limite de iteraÃ§Ãµes
      OPENHANDS_ENABLE_AUTO_LINT: true      # Auto-lint do cÃ³digo
    ports:
      - "3000:3000"
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      - ./openhands:/.openhands
      - ./workspace:/workspace
    extra_hosts:
      - "host.docker.internal:host-gateway"
    depends_on:
      runtime:
        condition: service_started
      ollama:
        condition: service_healthy  # âœ… Aguarda Ollama estar saudÃ¡vel
    networks:
      - openhands-net
    deploy:
      resources:
        limits:
          memory: 6g
          cpus: '4.0'
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    runtime: nvidia

  ollama:
    image: ollama/ollama:latest
    container_name: ollama-ai
    environment:
      # ConfiguraÃ§Ãµes otimizadas para Qwen2.5-Coder
      - OLLAMA_CONTEXT_LENGTH=32768
      - OLLAMA_HOST=0.0.0.0:11434
      - OLLAMA_MAX_LOADED_MODELS=1
      - OLLAMA_MODEL=qwen2.5-coder:7b        # âœ… MODELO OTIMIZADO
      - OLLAMA_GPU_LAYERS=35
      - OLLAMA_NUM_PARALLEL=1
      - OLLAMA_KEEP_ALIVE=24h                # âœ… Manter modelo carregado mais tempo
      # OtimizaÃ§Ãµes especÃ­ficas para codificaÃ§Ã£o
      - OLLAMA_FLASH_ATTENTION=1
      - OLLAMA_LOAD_TIMEOUT=900
      - OLLAMA_GPU_OVERHEAD=1073741824
      - OLLAMA_BATCH_SIZE=512                # âœ… Batch maior para codificaÃ§Ã£o
      - OLLAMA_F16=1
      - OLLAMA_PRELOAD=1
      # âœ… NOVAS OTIMIZAÃ‡Ã•ES
      - OLLAMA_TEMPERATURE=0.1               # Mais determinÃ­stico
      - OLLAMA_TOP_K=40
      - OLLAMA_TOP_P=0.9
      - OLLAMA_REPEAT_PENALTY=1.1
    ports:
      - "11434:11434"
    volumes:
      - ./ollama:/root/.ollama
      - ./workspace:/workspace
    restart: unless-stopped
    # âœ… HEALTHCHECK ADICIONADO
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/tags"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    entrypoint: ["/bin/sh", "-c"]
    command:
      - >
        ollama serve &
        sleep 10 &&
        echo "ðŸš€ Baixando modelo qwen2.5-coder:7b..." &&
        ollama pull qwen2.5-coder:7b &&
        echo "âœ… Modelo carregado com sucesso!" &&
        wait
    networks:
      - openhands-net
    extra_hosts:
      - "host.docker.internal:host-gateway"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    runtime: nvidia
        
  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: open-webui
    ports:
      - "8080:8080"
    volumes:
      - ./open-webui:/app/backend/data
      - ./workspace:/workspace
    environment:
      OLLAMA_BASE_URL: http://ollama:11434
      OPENWEBUI_USERNAME: admin@admin.com
      OPENWEBUI_PASSWORD: admin
      # âœ… CONFIGURAÃ‡Ã•ES ADICIONAIS
      ENABLE_MODEL_FILTER: true
      ENABLE_SIGNUP: false
      DEFAULT_MODELS: qwen2.5-coder:7b
    networks:
      - openhands-net
    extra_hosts:
      - "host.docker.internal:host-gateway"
    depends_on:
      ollama:
        condition: service_healthy           # âœ… Aguarda Ollama estar saudÃ¡vel
    deploy:
      resources:
        limits:
          memory: 2g
          cpus: '1.0'
        reservations:
          memory: 512m
          cpus: '0.5'

networks:
  openhands-net:
    name: openhands-net
