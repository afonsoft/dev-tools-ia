version: "3.9"

services:
  runtime:
    build:
      context: ./runtime
      dockerfile: Dockerfile
    image: openhands-runtime-dotnet
    container_name: openhands-runtime
    stdin_open: true
    tty: true
    volumes:
      - ./workspace:/workspace
    networks:
      - openhands-net
    extra_hosts:
      - "host.docker.internal:host-gateway"

  openhands:
    image: docker.all-hands.dev/all-hands-ai/openhands:0.55
    container_name: openhands-hands-app
    pull_policy: always
    environment:
      LOG_ALL_EVENTS: "true"
      # Configurações do LLM
      OPENHANDS_LLM_PROVIDER: ollama
      OPENHANDS_LLM_MODEL: codellama:7b
      OPENHANDS_LLM_ENDPOINT: http://ollama:11434
      OLLAMA_MODEL: codellama:7b
      # Otimizações GPU/CPU
      CUDA_VISIBLE_DEVICES: 0
      OPENHANDS_LLM_GPU_LAYERS: 35
      OPENHANDS_LLM_CTX_SIZE: 32768    # Aumentado para match com Ollama
      # Performance
      OPENHANDS_LOAD_TIMEOUT: 900
      OPENHANDS_MAX_PARALLEL_REQUESTS: 2
      OPENHANDS_MEMORY_BUDGET: 8589934592  # 8GB total memory budget
      SANDBOX_RUNTIME_CONTAINER_IMAGE: openhands-runtime-dotnet
    ports:
      - "3000:3000"
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      - ./openhands:/.openhands
      - ./workspace:/workspace
    extra_hosts:
      - "host.docker.internal:host-gateway"
    depends_on:
      runtime:
        condition: service_started
      ollama:
        condition: service_started
    networks:
      - openhands-net
    deploy:
      resources:
        limits:
          memory: 6g
          cpus: '4.0'
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    runtime: nvidia

  ollama:
    image: ollama/ollama:latest
    container_name: ollama-ai
    environment:
      # Configurações críticas para performance
      - OLLAMA_CONTEXT_LENGTH=32768    # Mínimo recomendado pela OpenHands
      - OLLAMA_HOST=0.0.0.0:11434
      - OLLAMA_MAX_LOADED_MODELS=1     # Otimizado para 8GB VRAM
      - OLLAMA_MODEL=codellama:7b
      - OLLAMA_GPU_LAYERS=35           # Ajustado para RTX 8GB
      - OLLAMA_NUM_PARALLEL=2          # Balanceado para sua CPU
      - OLLAMA_KEEP_ALIVE=-1
      # Otimizações de performance
      - OLLAMA_FLASH_ATTENTION=1       # Acelera processamento
      - OLLAMA_LOAD_TIMEOUT=900
      - OLLAMA_GPU_OVERHEAD=1073741824 # 1GB para overhead (metade do anterior)
      - OLLAMA_BATCH_SIZE=8            # Batch size otimizado
      - OLLAMA_F16=1                   # Usar FP16 para economia de VRAM
      - OLLAMA_PRELOAD=1               # Preload do modelo
    ports:
      - "11434:11434"
    volumes:
      - ./ollama:/root/.ollama
      - ./workspace:/workspace
    restart: unless-stopped
    entrypoint: ["/bin/sh", "-c"]
    command:
      - >
        ollama serve &
        sleep 5 &&
        ollama pull codellama:7b &&
        wait
    networks:
      - openhands-net
    extra_hosts:
      - "host.docker.internal:host-gateway"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    runtime: nvidia
        
  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: open-webui
    ports:
      - "8080:8080"
    volumes:
      - ./open-webui:/app/backend/data
      - ./workspace:/workspace
    environment:
      OLLAMA_BASE_URL: http://ollama:11434
      OPENWEBUI_USERNAME: admin@admin.com
      OPENWEBUI_PASSWORD: admin
    networks:
      - openhands-net
    extra_hosts:
      - "host.docker.internal:host-gateway"
    depends_on:
      - ollama
    deploy:
      resources:
        limits:
          memory: 2g
          cpus: '1.0'
        reservations:
          memory: 512m
          cpus: '0.5'

networks:
  openhands-net:
    name: openhands-net
