# Dev Tools IA

Ambiente de desenvolvimento integrado para processamento de linguagem natural e codifica√ß√£o assistida por IA, combinando OpenHands, Ollama e uma interface web amig√°vel.

## üìã Descri√ß√£o do Projeto

Este projeto fornece um ambiente de desenvolvimento containerizado focado em IA, integrando ferramentas avan√ßadas para desenvolvimento assistido por intelig√™ncia artificial. O ambiente oferece uma plataforma completa para desenvolvimento em .NET e Node.js, com suporte a processamento de linguagem natural e codifica√ß√£o assistida.

### üéØ Vis√£o de Neg√≥cio
- Aumento de produtividade atrav√©s de assist√™ncia IA no desenvolvimento
- Ambiente padronizado e reproduz√≠vel para toda a equipe
- Integra√ß√£o cont√≠nua com ferramentas modernas de desenvolvimento
- Suporte a m√∫ltiplos frameworks e tecnologias

### üîß Vis√£o T√©cnica
- Arquitetura baseada em containers com Docker
- Acelera√ß√£o por GPU para processamento de IA
- Modelo de linguagem local para maior seguran√ßa e privacidade
- Integra√ß√£o com ferramentas de desenvolvimento populares

## üìÅ Estrutura do Reposit√≥rio

```
.
‚îú‚îÄ‚îÄ openhands/           # Configura√ß√µes do OpenHands AI
‚îÇ   ‚îî‚îÄ‚îÄ settings.json    # Configura√ß√µes espec√≠ficas da IA
‚îú‚îÄ‚îÄ runtime/             # Ambiente de execu√ß√£o
‚îÇ   ‚îî‚îÄ‚îÄ Dockerfile      # Configura√ß√£o do container de runtime
‚îú‚îÄ‚îÄ vscode/             # Configura√ß√µes do VS Code
‚îÇ   ‚îú‚îÄ‚îÄ extensions.json # Extens√µes recomendadas
‚îÇ   ‚îî‚îÄ‚îÄ install-extensions.sh # Script de instala√ß√£o de extens√µes
‚îú‚îÄ‚îÄ docker-compose.yml  # Configura√ß√£o dos servi√ßos
‚îú‚îÄ‚îÄ configure.sh        # Script de configura√ß√£o do ambiente
‚îú‚îÄ‚îÄ start.sh           # Script de inicializa√ß√£o
‚îî‚îÄ‚îÄ LICENSE            # Licen√ßa do projeto
```

## üõ†Ô∏è Stack Tecnol√≥gica

### Runtime e Infraestrutura
- **Base**: Docker com suporte NVIDIA GPU
- **Container Runtime**: Docker Compose
- **GPU Support**: NVIDIA Container Toolkit

### Frameworks e SDKs
- **.NET**
  - SDK 8.0 (LTS)
  - SDK 9.0 (Preview)
- **Node.js**
  - Gerenciamento via NVM
  - √öltima vers√£o LTS

### Componentes Principais
- **[OpenHands](https://github.com/all-hands-dev/openhands)**
  - Vers√£o: 0.58 (√∫ltima est√°vel)
  - Ambiente de desenvolvimento IA aprimorado
  - Integra√ß√£o com VS Code
  - Suporte a m√∫ltiplos modelos
  - Otimiza√ß√£o de mem√≥ria melhorada
  - Interface local: http://localhost:3000
- **[Ollama](https://github.com/ollama/ollama)**
  - API de modelo de linguagem
  - Modelo padr√£o: Devstral 24B
  - Especializado em desenvolvimento e agentes de c√≥digo
  - Otimizado para alta performance
  - Suporte a m√∫ltiplos modelos via API
  - API local: http://localhost:11434
- **[Open WebUI](https://github.com/open-webui/open-webui)**
  - Interface gr√°fica amig√°vel
  - Autentica√ß√£o integrada
  - Customiza√ß√£o de par√¢metros
  - Interface local: http://localhost:8080

## üß© Dockerfile ‚Äî `runtime`

Resumo t√©cnico do arquivo `runtime/Dockerfile` (fonte: `runtime/Dockerfile`):

- **Imagem base**: `docker.openhands.dev/openhands/runtime:0.62-nikolaik` ‚Äî imagem preparada com runtime e ferramentas de base.
- **Locale**: `LANG=C.UTF-8`, `LC_ALL=C.UTF-8` para evitar warnings relacionados a locale.
- **Depend√™ncias instaladas (apt)**: `ca-certificates`, `curl`, `ffmpeg`, `git`, `libbz2-dev`, `libffi-dev`, `libfontconfig1`, `libfreetype6`, `liblzma-dev`, `libncursesw5-dev`, `libreadline-dev`, `libsqlite3-dev`, `libssl-dev`, `libxml2-dev`, `libxmlsec1-dev`, `python3-pip`, `python3-setuptools`, `python3-venv`, `zlib1g-dev`.
- **Node.js / NVM**: instala `nvm` (v0.40.3), instala a vers√£o LTS do Node.js via `nvm install --lts` e configura `nvm alias default`.
- **.NET SDKs**: usa `dotnet-install.sh` para instalar os canais `8` e `10` em `/usr/share/dotnet` e cria symlink para `/usr/bin/dotnet`.
- **Python venv e pacotes OpenHands**: cria venv em `/opt/venv`, atualiza `pip/setuptools` e instala `openhands-agent-server`, `openhands-sdk`, `openhands-tools`, `openhands-workspace`.
- **Astral UV e ferramentas**: instala o instalador `astral.sh/uv` e registra `openhands` como ferramenta (`uv tool install openhands`).
- **Configura√ß√µes de PATH e vari√°veis**:
  - `NVM_DIR=/root/.nvm`
  - `PATH` atualizado para incluir Node, .NET tools e `/opt/venv/bin` e `/root/.local/bin`
  - `DOTNET_ROOT=/usr/share/dotnet`
  - `DOTNET_CLI_TELEMETRY_OPTOUT=true`
  - `NODE_TLS_REJECT_UNAUTHORIZED=0`
- **Ajustes de Git/NPM**: desabilita verifica√ß√£o SSL global do Git (`git config --global http.sslVerify false`) e `npm config set strict-ssl false` (√∫til em ambientes controlados, N√ÉO recomendado para ambientes p√∫blicos sem proxy seguro).
- **Diret√≥rio de trabalho**: `WORKDIR /app`.
- **Boas pr√°ticas aplicadas no Dockerfile**:
  - Agrupamento de comandos `apt-get` e limpeza (`apt-get clean`, `rm -rf /var/lib/apt/lists/*`) para reduzir tamanho da imagem.
  - Uso de virtualenv para isolar pacotes Python e evitar PEP 668 problems.
  - Notas no Dockerfile recomendam avaliar instala√ß√£o direta do Node.js para produ√ß√£o em vez de `nvm`.

Recomenda√ß√µes r√°pidas
- Revise `npm` e `git` configs para ambientes p√∫blicos (n√£o desabilitar SSL em produ√ß√£o).
- Se for publicar imagem para produ√ß√£o, prefira instalar Node.js direto em vez de `nvm` (evita complexidade em imagens n√£o-interativas).
- Considere expor vari√°veis sens√≠veis externamente via `docker-compose` ou secrets em vez de hardcode no Dockerfile.


## üöÄ Como Rodar a Aplica√ß√£o

1. **Pr√©-requisitos**
   - Docker com suporte a NVIDIA GPU
   - NVIDIA Container Toolkit instalado
   - M√≠nimo de 16GB RAM (32GB recomendado)
   - GPU com 8GB+ VRAM

2. **Instala√ß√£o**
   ```bash
   # Clone o reposit√≥rio
   git clone https://github.com/afonsoft/dev-tools-ia.git
   cd dev-tools-ia

   # Configure o ambiente
   ./configure.sh

   # Inicie os servi√ßos
   ./start.sh
   ```

3. **Acesso √†s Interfaces**
   - OpenHands UI: http://localhost:3000
   - Web UI: http://localhost:8080
   - Ollama API: http://localhost:11434

## ‚öôÔ∏è Vari√°veis de Ambiente

| Vari√°vel | Descri√ß√£o | Valor Padr√£o |
|----------|-----------|--------------|
| OPENHANDS_LLM_PROVIDER | Provedor do modelo LLM | ollama |
| OPENHANDS_LLM_MODEL | Modelo LLM a ser usado | devstral:24b |
| OPENHANDS_LLM_TEMPERATURE | Temperatura de gera√ß√£o | 0.1 |
| OPENHANDS_LLM_CTX_SIZE | Tamanho do contexto | 32768 |
| CUDA_VISIBLE_DEVICES | GPU a ser utilizada | 0 |
| OPENHANDS_MAX_PARALLEL_REQUESTS | Requisi√ß√µes paralelas | 1 |

## üìà Status do Projeto

Status: **Conclu√≠do**

Para mais detalhes sobre as √∫ltimas altera√ß√µes, consulte o [CHANGELOG.md](CHANGELOG.md).

## üìÑ Licen√ßa

Este projeto est√° sob a licen√ßa [GNU GPL v3.0](LICENSE).

---
*√öltima atualiza√ß√£o: 2025-11-11*

Atualiza√ß√µes: README e CHANGELOG revisados; altera√ß√µes preparadas na branch `feature/update`.

1. **Devstral (24B)** - Modelo Padr√£o
   - Especializado em tarefas de desenvolvimento e agentes de c√≥digo
   - Performance superior em tarefas complexas
   - Requisitos: 32GB+ RAM, 12GB+ VRAM
   - Uso: Desenvolvimento profissional e projetos complexos

2. **CodeLlama (7B/13B/34B)**
   - Focado em gera√ß√£o e an√°lise de c√≥digo
   - Suporte a m√∫ltiplas linguagens de programa√ß√£o
   - Requisitos: 16GB+ RAM, 8GB+ VRAM (vers√£o 7B)
   - Uso: Desenvolvimento geral e an√°lise de c√≥digo

3. **Qwen2.5-Coder (7B/14B/32B)**
   - Especializado em c√≥digo com contexto longo
   - Suporte a at√© 128K tokens
   - Requisitos: 16GB+ RAM, 8GB+ VRAM (vers√£o 7B)
   - Uso: Projetos com bases de c√≥digo grandes

#### Modelos para Hardware Limitado

1. **Phi (2.7B/3.8B)**
   - Modelo leve com boa performance
   - Excelente rela√ß√£o tamanho/performance
   - Requisitos: 8GB+ RAM, 4GB+ VRAM
   - Uso: Desenvolvimento em hardware modesto

2. **TinyLlama (1.1B)**
   - Modelo ultra-leve
   - Bom para tarefas b√°sicas
   - Requisitos: 6GB+ RAM, 4GB+ VRAM
   - Uso: Ambientes com recursos muito limitados

#### Modelos para Agentes de C√≥digo

1. **GPT-OSS (20B/120B)**
   - Especializado em tarefas de agente e racioc√≠nio
   - Excelente para casos de uso de desenvolvimento
   - Requisitos: 32GB+ RAM, 16GB+ VRAM (vers√£o 20B)
   - Uso: Desenvolvimento complexo e automa√ß√£o

2. **DeepSeek-R1 (7B/32B/70B)**
   - Performance pr√≥xima a modelos l√≠deres
   - Excelente capacidade de racioc√≠nio
   - Requisitos: 16GB+ RAM, 8GB+ VRAM (vers√£o 7B)
   - Uso: Tarefas de agente e desenvolvimento

3. **Qwen3 (8B/14B/30B)**
   - Suite completa para desenvolvimento
   - Suporte a MoE (Mixture of Experts)
   - Requisitos: 16GB+ RAM, 8GB+ VRAM (vers√£o 8B)
   - Uso: Gera√ß√£o de c√≥digo e automa√ß√£o

4. **LLama3.1 Groq Tool-Use (8B/70B)**
   - Otimizado para uso de ferramentas
   - Excelente para function calling
   - Requisitos: 16GB+ RAM, 8GB+ VRAM (vers√£o 8B)
   - Uso: Integra√ß√£o com ferramentas e APIs

5. **Mistral-Small3.2 (24B)**
   - Function calling aprimorado
   - Melhor seguimento de instru√ß√µes
   - Requisitos: 24GB+ RAM, 12GB+ VRAM
   - Uso: Agentes de c√≥digo precisos

#### Modelos Especializados em Desenvolvimento

1. **Mistral-Large (123B)**
   - L√≠der em gera√ß√£o de c√≥digo
   - Contexto de 128k tokens
   - Suporte multil√≠ngue avan√ßado
   - Requisitos: 48GB+ RAM, 24GB+ VRAM
   - Uso: Projetos grandes e complexos

2. **CodeLlama (7B/13B)**
   - Otimizado para desenvolvimento e an√°lise de c√≥digo
   - Excelente suporte multilinguagem
   - Requisitos: 8GB+ RAM, 6GB+ VRAM (vers√£o 7B)
   - Uso: Projetos pequenos e m√©dios
   - Dispon√≠vel em variantes: base, instruct, python

3. **Devstral (24B)**
   - Especializado em agentes de c√≥digo
   - √ìtimo para automa√ß√£o de desenvolvimento
   - Requisitos: 32GB+ RAM, 16GB+ VRAM
   - Uso: Automa√ß√£o de desenvolvimento

> **Nota sobre Sele√ß√£o de Modelos**: Para tarefas que envolvem agentes de c√≥digo e automa√ß√£o de desenvolvimento, recomendamos come√ßar com modelos menores como Granite3-Dense (2B) ou LLama3.1 (8B) e escalar conforme necess√°rio. Modelos maiores como GPT-OSS (120B) e Mistral-Large (123B) oferecem melhor performance mas requerem significativamente mais recursos. Considere o trade-off entre performance e recursos dispon√≠veis ao escolher o modelo.

#### Configura√ß√µes de Otimiza√ß√£o

##### Configura√ß√µes do Ollama
- `OLLAMA_CONTEXT_LENGTH`: 32768 (recomendado para boa performance)
- `OLLAMA_GPU_LAYERS`: Ajuste baseado na VRAM dispon√≠vel
  - 35 layers para GPUs com 8GB VRAM
  - 45 layers para GPUs com 12GB+ VRAM
- `OLLAMA_MAX_LOADED_MODELS`: 1 (otimizar uso de VRAM)
- `OLLAMA_GPU_OVERHEAD`: Buffer de GPU
  - 1GB para GPUs com 8GB VRAM
  - 2GB para GPUs com 12GB+ VRAM
- `OLLAMA_NUM_PARALLEL`: 2 (balanceado para maioria dos sistemas)
- `OLLAMA_FLASH_ATTENTION`: 1 (acelera processamento)
- `OLLAMA_F16`: 1 (economia de VRAM com FP16)
- `OLLAMA_BATCH_SIZE`: 8 (otimizado para performance)
- `OLLAMA_PRELOAD`: 1 (carregamento mais r√°pido)

##### Configura√ß√µes do OpenHands
- `OPENHANDS_LLM_CTX_SIZE`: 32768 (match com Ollama)
- `OPENHANDS_LLM_GPU_LAYERS`: Mesmo valor do Ollama
- `OPENHANDS_MAX_PARALLEL_REQUESTS`: 2 (balanceado)
- `OPENHANDS_MEMORY_BUDGET`: Ajuste baseado na RAM
  - 8GB para sistemas com 12GB RAM
  - 12GB para sistemas com 16GB+ RAM

##### Configura√ß√µes Recomendadas por Hardware

1. **Sistema B√°sico (8GB RAM, GPU 6GB VRAM)**
   ```yaml
   OLLAMA_GPU_LAYERS: 25
   OLLAMA_GPU_OVERHEAD: 536870912  # 512MB
   OLLAMA_NUM_PARALLEL: 1
   OPENHANDS_MEMORY_BUDGET: 6442450944  # 6GB
   ```

2. **Sistema Intermedi√°rio (12GB RAM, GPU 8GB VRAM)**
   ```yaml
   OLLAMA_GPU_LAYERS: 35
   OLLAMA_GPU_OVERHEAD: 1073741824  # 1GB
   OLLAMA_NUM_PARALLEL: 2
   OPENHANDS_MEMORY_BUDGET: 8589934592  # 8GB
   ```

3. **Sistema Avan√ßado (16GB+ RAM, GPU 12GB+ VRAM)**
   ```yaml
   OLLAMA_GPU_LAYERS: 45
   OLLAMA_GPU_OVERHEAD: 2147483648  # 2GB
   OLLAMA_NUM_PARALLEL: 3
   OPENHANDS_MEMORY_BUDGET: 12884901888  # 12GB
   ```

> **Nota**: Estas configura√ß√µes s√£o pontos de partida recomendados. Ajuste com base no comportamento do sistema e nas necessidades espec√≠ficas do seu projeto.

##### Configura√ß√µes Otimizadas para GPUs RTX S√©rie 30/40

###### RTX 3050 (8GB VRAM)
```yaml
# Configura√ß√µes Ollama
OLLAMA_GPU_LAYERS: 38              # Otimizado para arquitetura Ampere
OLLAMA_GPU_OVERHEAD: 1073741824    # 1GB overhead
OLLAMA_NUM_PARALLEL: 2
OLLAMA_BATCH_SIZE: 12              # Maior batch size devido ao melhor scheduler
OLLAMA_F16: 1                      # Habilita FP16 para economia de VRAM
OLLAMA_FLASH_ATTENTION: 1
OLLAMA_CONCURRENT_SLOTS: 2         # Slots de execu√ß√£o paralela

# Configura√ß√µes OpenHands
OPENHANDS_LLM_GPU_LAYERS: 38
OPENHANDS_MEMORY_BUDGET: 10737418240  # 10GB para sistemas com 16GB+ RAM
OPENHANDS_MAX_PARALLEL_REQUESTS: 2
```

###### RTX 4050 (6GB/8GB VRAM)
```yaml
# Configura√ß√µes Ollama
OLLAMA_GPU_LAYERS: 42              # Otimizado para arquitetura Ada Lovelace
OLLAMA_GPU_OVERHEAD: 1073741824    # 1GB overhead
OLLAMA_NUM_PARALLEL: 2
OLLAMA_BATCH_SIZE: 16              # Maior devido ao melhor tensor core
OLLAMA_F16: 1                      # Habilita FP16
OLLAMA_FLASH_ATTENTION: 1
OLLAMA_CONCURRENT_SLOTS: 3         # Mais slots devido √† melhor efici√™ncia
OLLAMA_TENSOR_SPLIT: 1             # Habilita tensor splitting

# Configura√ß√µes OpenHands
OPENHANDS_LLM_GPU_LAYERS: 42
OPENHANDS_MEMORY_BUDGET: 12884901888  # 12GB para sistemas com 16GB+ RAM
OPENHANDS_MAX_PARALLEL_REQUESTS: 3
```

> **Benef√≠cios Espec√≠ficos por GPU**:
> 
> **RTX 3050**:
> - Melhor suporte a FP16 com Ampere
> - Bom desempenho em batch processing
> - Permite at√© 38 camadas na GPU com seguran√ßa
> 
> **RTX 4050**:
> - Arquitetura Ada mais eficiente
> - Melhor performance em FP16
> - Suporte a tensor splitting
> - Permite at√© 42 camadas na GPU
> 
> **Dicas de Otimiza√ß√£o**:
> 1. Monitore a temperatura da GPU (ideal < 80¬∞C)
> 2. Use `nvidia-smi` para monitorar uso de VRAM
> 3. Ajuste `OLLAMA_GPU_LAYERS` se observar OOM (Out of Memory)
> 4. Reduza `BATCH_SIZE` se encontrar instabilidades
> 
> **Modelos Recomendados**:
> - CodeLlama 7B (est√°vel e eficiente)
> - Mistral 7B (boa performance)
> - Llama2 7B (base confi√°vel)

##### Configura√ß√µes Otimizadas por Quantidade de RAM

###### Sistema com 16GB RAM
```yaml
# Configura√ß√µes de Mem√≥ria Gerais
OPENHANDS_MEMORY_BUDGET: 10737418240       # 10GB para OpenHands
deploy:
  resources:
    limits:
      memory: 8g                           # Limite OpenHands
    reservations:
      memory: 6g                           # Reserva m√≠nima

# Configura√ß√µes Ollama
OLLAMA_MAX_LOADED_MODELS: 1
OLLAMA_GPU_OVERHEAD: 1073741824            # 1GB para GPU overhead
OLLAMA_BATCH_SIZE: 8
OLLAMA_NUM_PARALLEL: 2

# Modelos Recomendados:
# - CodeLlama 7B (padr√£o)
# - Mistral 7B
# - Llama2 7B
```

###### Sistema com 20GB RAM
```yaml
# Configura√ß√µes de Mem√≥ria Gerais
OPENHANDS_MEMORY_BUDGET: 14737418240       # 14GB para OpenHands
deploy:
  resources:
    limits:
      memory: 12g                          # Limite OpenHands
    reservations:
      memory: 8g                           # Reserva m√≠nima

# Configura√ß√µes Ollama
OLLAMA_MAX_LOADED_MODELS: 1
OLLAMA_GPU_OVERHEAD: 1610612736            # 1.5GB para GPU overhead
OLLAMA_BATCH_SIZE: 12
OLLAMA_NUM_PARALLEL: 2

# Modelos Recomendados:
# - Mistral 8B
# - DeepSeek-Coder 6.7B
# - Qwen 7B
```

###### Sistema com 32GB RAM
```yaml
# Configura√ß√µes de Mem√≥ria Gerais
OPENHANDS_MEMORY_BUDGET: 21474836480       # 20GB para OpenHands
deploy:
  resources:
    limits:
      memory: 16g                          # Limite OpenHands
    reservations:
      memory: 12g                          # Reserva m√≠nima

# Configura√ß√µes Ollama
OLLAMA_MAX_LOADED_MODELS: 2                # Permite carregar 2 modelos
OLLAMA_GPU_OVERHEAD: 2147483648            # 2GB para GPU overhead
OLLAMA_BATCH_SIZE: 16
OLLAMA_NUM_PARALLEL: 3
OLLAMA_CONCURRENT_SLOTS: 4

# Modelos Recomendados:
# - Devstral 24B
# - CodeLlama 13B
# - DeepSeek-Coder 16B
```

###### Sistema com 64GB RAM
```yaml
# Configura√ß√µes de Mem√≥ria Gerais
OPENHANDS_MEMORY_BUDGET: 42949672960       # 40GB para OpenHands
deploy:
  resources:
    limits:
      memory: 32g                          # Limite OpenHands
    reservations:
      memory: 24g                          # Reserva m√≠nima

# Configura√ß√µes Ollama
OLLAMA_MAX_LOADED_MODELS: 3                # Permite carregar 3 modelos
OLLAMA_GPU_OVERHEAD: 4294967296            # 4GB para GPU overhead
OLLAMA_BATCH_SIZE: 24
OLLAMA_NUM_PARALLEL: 4
OLLAMA_CONCURRENT_SLOTS: 6
OLLAMA_PRELOAD: 1
OLLAMA_NUMA: 1                            # Habilita otimiza√ß√µes NUMA

# Modelos Recomendados:
# - Mistral-Large 32B
# - CodeLlama 34B
# - Qwen 72B
```

> **Notas de Otimiza√ß√£o por RAM**:
>
> **16GB RAM**:
> - Foco em modelos at√© 8B
> - Gerenciamento conservador de mem√≥ria
> - Priorize um √∫nico modelo carregado
>
> **20GB RAM**:
> - Suporte a modelos at√© 13B
> - Melhor capacidade de processamento paralelo
> - Boa para desenvolvimento m√©dio
>
> **32GB RAM**:
> - Ideal para modelos at√© 24B
> - Suporte a m√∫ltiplos modelos
> - Excelente para desenvolvimento profissional
>
> **64GB RAM**:
> - Suporte a modelos grandes (30B+)
> - M√∫ltiplos modelos em paralelo
> - Otimiza√ß√µes NUMA habilitadas
> - Ideal para ambientes de produ√ß√£o
>
> **Dicas Gerais**:
> 1. Reserve 20-30% da RAM total para o sistema operacional
> 2. Use `OLLAMA_PRELOAD` em sistemas com RAM abundante
> 3. Ajuste `NUMA` apenas em servidores multi-socket
> 4. Monitore swap com `free -h` (Linux) ou Gerenciador de Tarefas (Windows)
> 5. Em caso de OOM, reduza `OPENHANDS_MEMORY_BUDGET` em 25%
> 6. Com OpenHands 0.56+, use configura√ß√µes de paralelismo conservadoras
> 7. Para Devstral 24B, recomenda-se m√≠nimo de 32GB RAM
>
> **Importante: Gerenciamento de Paralelismo**
> - `OLLAMA_NUM_PARALLEL` e `OPENHANDS_MAX_PARALLEL_REQUESTS` afetam diretamente o consumo de mem√≥ria
> - Cada requisi√ß√£o paralela adicional aumenta significativamente o uso de RAM
> - Recomenda√ß√µes por configura√ß√£o de mem√≥ria:
>   - 8-12GB RAM: mantenha ambos em 1
>   - 16GB RAM: m√°ximo de 2 paralelos
>   - 32GB+ RAM: pode usar 3-4 paralelos
> - Se encontrar erros de mem√≥ria, reduza estes valores para 1 primeiro
> - Aumente apenas se tiver certeza que h√° mem√≥ria dispon√≠vel

#### Alternativa Recomendada: CodeLlama 7B
O [CodeLlama](https://ai.meta.com/blog/code-llama-large-language-model-coding/) √© uma excelente alternativa para sistemas com recursos mais limitados:

- **Capacidades**:
  - Compreens√£o e gera√ß√£o de c√≥digo em m√∫ltiplas linguagens
  - Completa√ß√£o de c√≥digo inteligente
  - Explica√ß√£o de c√≥digo existente
  - Refatora√ß√£o e documenta√ß√£o

- **Vantagens**:
  - Menor consumo de mem√≥ria
  - Bom desempenho em GPUs mais modestas
  - Equil√≠brio entre performance e recursos

- **Requisitos M√≠nimos**:
  - RAM: 16GB
  - GPU: 8GB VRAM (ou CPU razo√°vel)
  - SSD: 20GB livre

## Pr√©-requisitos

### Hardware
- CPU: 4+ cores recomendados
- RAM: 16GB+ recomendados
- GPU: NVIDIA com suporte a CUDA (opcional, mas recomendado)
- Armazenamento: 20GB+ livres

### Software
- Docker Engine 24.0+
- Docker Compose v2.20+
- NVIDIA Driver 525+ (para GPU)
- NVIDIA Container Toolkit
- Git 2.40+

## Guia de Instala√ß√£o

1. Clone o reposit√≥rio:
   ```bash
   git config --global http.sslVerify false  # Se necess√°rio
   git clone [URL_DO_REPOSITORIO]
   ```

2. Configure o ambiente:
   ```bash
   # Para Windows
   git config --global core.longpaths true
   
   # Para NPM (se necess√°rio)
   npm config set strict-ssl false
   ```

3. Execute o script de inicializa√ß√£o:
   - Linux/macOS/Windows: `./start.sh`

O ambiente ser√° iniciado automaticamente e abrir√°:
- OpenHands UI: http://localhost:3000
- Web Chat UI: http://localhost:8080

## Estrutura do Reposit√≥rio

```
.
‚îú‚îÄ‚îÄ openhands/                # Configura√ß√µes do OpenHands
‚îÇ   ‚îî‚îÄ‚îÄ settings.json        # Configura√ß√µes do agente e LLM
‚îú‚îÄ‚îÄ runtime/                 # Ambiente de execu√ß√£o
‚îÇ   ‚îî‚îÄ‚îÄ Dockerfile          # Configura√ß√£o do container base
‚îú‚îÄ‚îÄ vscode/                 # Configura√ß√µes do VS Code
‚îÇ   ‚îú‚îÄ‚îÄ extensions.json     # Extens√µes recomendadas
‚îÇ   ‚îî‚îÄ‚îÄ install-extensions.sh # Instalador de extens√µes
‚îú‚îÄ‚îÄ docker-compose.yml      # Defini√ß√£o dos servi√ßos
‚îú‚îÄ‚îÄ start.bat              # Inicializa√ß√£o (Windows)
‚îú‚îÄ‚îÄ start.sh              # Inicializa√ß√£o (Unix)
‚îú‚îÄ‚îÄ .gitignore            # Exclus√µes do Git
‚îú‚îÄ‚îÄ LICENSE              # GNU GPL v3.0
‚îú‚îÄ‚îÄ CHANGELOG.md         # Registro de altera√ß√µes
‚îî‚îÄ‚îÄ README.md           # Esta documenta√ß√£o
```

## Configura√ß√£o e Vari√°veis de Ambiente

### Configura√ß√£o do Modelo LLM

O ambiente suporta diferentes modelos LLM que podem ser configurados de acordo com os recursos dispon√≠veis em sua m√°quina. Para alterar o modelo, edite as configura√ß√µes no arquivo `openhands/settings.json` e `docker-compose.yml`.

#### Cen√°rios de Uso

1. **Configura√ß√£o Padr√£o (Hardware Robusto)**
```json
// openhands/settings.json
{
    "llm_model": "ollama/devstral:latest",
    "llm_base_url": "http://ollama:11434"
}
```
```yaml
# docker-compose.yml (se√ß√£o ollama)
environment:
  - OLLAMA_MODEL=devstral:latest
  - OLLAMA_CONTEXT_LENGTH=32768
  - OLLAMA_GPU_LAYERS=80
  - OLLAMA_MAX_LOADED_MODELS=1
  - OLLAMA_FLASH_ATTENTION=1
  - OLLAMA_GPU_OVERHEAD=2147483648  # 2GB para buffer de GPU
```

2. **Configura√ß√£o Recomendada (Hardware Moderado)**
```json
// openhands/settings.json
{
    "llm_model": "ollama/codellama:7b",
    "llm_base_url": "http://ollama:11434"
}
```
```yaml
# docker-compose.yml (se√ß√£o ollama)
environment:
  - OLLAMA_MODEL=codellama:7b
  - OLLAMA_CONTEXT_LENGTH=16384
  - OLLAMA_GPU_LAYERS=45
  - OLLAMA_MAX_LOADED_MODELS=1
  - OLLAMA_FLASH_ATTENTION=1
  - OLLAMA_GPU_OVERHEAD=1073741824  # 1GB para buffer de GPU
```

O CodeLlama 7B √© recomendado para a maioria dos usu√°rios por:
- Menor consumo de recursos
- Boa performance em hardware mais modesto
- Excelente suporte a m√∫ltiplas linguagens

2. **Recursos Limitados (8-16GB RAM, GPU < 8GB)**
```json
// openhands/settings.json
{
    "llm_model": "ollama/phi:latest",  // ou "ollama/neural-chat:latest"
    "llm_base_url": "http://ollama:11434"
}
```
```yaml
# docker-compose.yml (se√ß√£o ollama)
environment:
  - OLLAMA_MODEL=phi:latest
  - OLLAMA_CONTEXT_LENGTH=8192
  - OLLAMA_GPU_LAYERS=35  # Ajuste conforme necess√°rio
  - OLLAMA_MAX_LOADED_MODELS=1
```

3. **Recursos Moderados (16-32GB RAM, GPU 8-12GB)**
```json
// openhands/settings.json
{
    "llm_model": "ollama/mistral:latest",  // ou "ollama/codellama:7b"
    "llm_base_url": "http://ollama:11434"
}
```
```yaml
# docker-compose.yml (se√ß√£o ollama)
environment:
  - OLLAMA_MODEL=mistral:latest
  - OLLAMA_CONTEXT_LENGTH=16384
  - OLLAMA_GPU_LAYERS=45
  - OLLAMA_MAX_LOADED_MODELS=1
```

4. **Recursos Abundantes (32GB+ RAM, GPU 16GB+)**
```json
// openhands/settings.json
{
    "llm_model": "ollama/qwen2.5-coder:7b",  // ou "ollama/codellama:13b"
    "llm_base_url": "http://ollama:11434"
}
```
```yaml
# docker-compose.yml (se√ß√£o ollama)
environment:
  - OLLAMA_MODEL=qwen2.5-coder:7b
  - OLLAMA_CONTEXT_LENGTH=32768
  - OLLAMA_GPU_LAYERS=80
  - OLLAMA_MAX_LOADED_MODELS=1
```

#### Par√¢metros de Otimiza√ß√£o
- `OLLAMA_CONTEXT_LENGTH`: Tamanho do contexto (menor = menos mem√≥ria)
- `OLLAMA_GPU_LAYERS`: N√∫mero de camadas na GPU (mais = melhor performance, mais VRAM)
- `OLLAMA_MAX_LOADED_MODELS`: Limite de modelos carregados simultaneamente
- `OLLAMA_GPU_OVERHEAD`: Buffer de mem√≥ria GPU (default: 2GB)

#### Alterando o Modelo via Script

O projeto inclui um script `configure.sh` que facilita a troca do modelo LLM. Para utiliz√°-lo:

```bash
# Dar permiss√£o de execu√ß√£o (necess√°rio apenas uma vez)
chmod +x configure.sh

# Alterar para um modelo leve (8-16GB RAM)
./configure.sh phi:latest

# Alterar para um modelo intermedi√°rio (16-32GB RAM)
./configure.sh codellama:7b

# Alterar para um modelo mais robusto (32GB+ RAM)
./configure.sh qwen2.5-coder:7b

# Ap√≥s alterar o modelo, reinicie os containers
docker-compose down && docker-compose up -d
```

O script automaticamente:
1. Atualiza o modelo no `docker-compose.yml`
2. Configura o `settings.json` do OpenHands
3. Ajusta o comando de pull do Ollama
4. Fornece instru√ß√µes para reiniciar os containers

> **Dica**: Para ver os modelos dispon√≠veis, visite o [Ollama Model Library](https://ollama.ai/library)

### OpenHands
```env
LOG_ALL_EVENTS=true
LLM_MAX_INPUT_TOKENS=16384
LLM_MAX_OUTPUT_TOKENS=16384
OPENHANDS_LLM_PROVIDER=ollama
OPENHANDS_LLM_MODEL=qwen2.5-coder:7b
```

### Ollama
```env
OLLAMA_CONTEXT_LENGTH=32768
OLLAMA_HOST=0.0.0.0:11434
OLLAMA_MODEL=qwen2.5-coder:7b
OLLAMA_NUM_PARALLEL=1
OLLAMA_KEEP_ALIVE=-1
OLLAMA_FLASH_ATTENTION=1
```

### Web UI
```env
OPENWEBUI_USERNAME=admin@admin.com
OPENWEBUI_PASSWORD=admin
```

## Extens√µes VS Code

### Desenvolvimento .NET
- [C# Dev Kit](https://marketplace.visualstudio.com/items?itemName=ms-dotnettools.csdevkit)
- [C# Extensions Pack](https://marketplace.visualstudio.com/items?itemName=boundarystudio.csharp-extentions-pack)

### Git e Controle de Vers√£o
- [Git Extension Pack](https://marketplace.visualstudio.com/items?itemName=donjayamanne.git-extension-pack)
- [GitHub Actions](https://marketplace.visualstudio.com/items?itemName=github.vscode-github-actions)
- [GitHub Pull Requests](https://marketplace.visualstudio.com/items?itemName=github.vscode-pull-request-github)

### IA e Desenvolvimento
- [GitHub Copilot](https://marketplace.visualstudio.com/items?itemName=github.copilot)
- [Ollama VSCode](https://marketplace.visualstudio.com/items?itemName=genepiot.ollama-vscode)

### Formata√ß√£o e Qualidade
- [EditorConfig](https://marketplace.visualstudio.com/items?itemName=editorconfig.editorconfig)
- [Prettier](https://marketplace.visualstudio.com/items?itemName=esbenp.prettier-vscode)
- [ESLint](https://marketplace.visualstudio.com/items?itemName=dbaeumer.vscode-eslint)

### Frameworks e UI
- [Angular Extension Pack](https://marketplace.visualstudio.com/items?itemName=johnpapa.angular2)
- [Tailwind CSS IntelliSense](https://marketplace.visualstudio.com/items?itemName=bradlc.vscode-tailwindcss)

## Recursos do Sistema

### Limites por Container
- **OpenHands**
  - Mem√≥ria: 2-4GB
  - CPU: 1-2 cores
- **Ollama**
  - GPU: Acesso total
  - VRAM: 2GB overhead
- **Web UI**
  - Mem√≥ria: 512MB-2GB
  - CPU: 0.5-1 core

## Status do Projeto

- **Estado**: Conclu√≠do
- **Vers√£o**: 1.3.0
- **√öltima Atualiza√ß√£o**: 11 de Novembro de 2025

## Problemas Conhecidos e Solu√ß√µes

### Erro de Mem√≥ria Insuficiente no Ollama

Se voc√™ encontrar o seguinte erro nos logs do container Ollama:
```
error="model requires more system memory (33.0 GiB) than is available (10.3 GiB)"
```

Este erro ocorre quando o modelo LLM selecionado requer mais mem√≥ria do que est√° dispon√≠vel no seu sistema. Para resolver:

1. **Solu√ß√£o Imediata**: Mude para um modelo que requer menos mem√≥ria
   ```bash
   # Pare os containers
   docker compose down

   # Edite openhands/settings.json
   # Altere o modelo para uma op√ß√£o mais leve:
   {
     "llm_model": "ollama/phi:latest",  # Modelo leve (~4GB RAM)
     # ou
     "llm_model": "ollama/mistral:latest"  # Modelo moderado (~8GB RAM)
   }

   # Edite docker-compose.yml
   # Na se√ß√£o 'ollama', atualize:
   environment:
     - OLLAMA_MODEL=phi:latest  # ou mistral:latest
     - OLLAMA_CONTEXT_LENGTH=8192  # Reduzir contexto tamb√©m ajuda
   
   # Reinicie os containers
   docker compose up -d
   ```

2. **Modelos Recomendados por Faixa de Mem√≥ria**:
   - 8GB RAM: `phi:latest`, `neural-chat:latest`
   - 16GB RAM: `mistral:latest`, `codellama:7b`
   - 32GB+ RAM: `qwen2.5-coder:7b`, `codellama:13b`

3. **Otimiza√ß√µes Adicionais**:
   - Reduza `OLLAMA_CONTEXT_LENGTH` para 8192 ou 4096
   - Defina `OLLAMA_MAX_LOADED_MODELS=1`
   - Aumente o swap do sistema se poss√≠vel
   - Feche aplica√ß√µes desnecess√°rias

Para mais detalhes sobre configura√ß√£o de modelos, consulte a se√ß√£o "Configura√ß√£o do Modelo LLM" acima.

## Licen√ßa

Este projeto est√° licenciado sob a [GNU General Public License v3.0](LICENSE).

## Links √öteis

- [Conventional Commits](https://www.conventionalcommits.org/pt-br/v1.0.0/)
- [Keep a Changelog](https://keepachangelog.com/pt-BR/1.1.0/)
- [Versionamento Sem√¢ntico](https://semver.org/lang/pt-BR/)
- [Documenta√ß√£o Docker](https://docs.docker.com/)
- [Documenta√ß√£o .NET](https://learn.microsoft.com/dotnet/)

[Ver Changelog](CHANGELOG.md)
